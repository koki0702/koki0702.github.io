<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="../favicon.ico">
  <title>Step 4: Numerical Differentiation &mdash; DeZero Book  </title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Mono"><link rel="stylesheet" href="../static/typlog.css?v=0.7.3" type="text/css" />
  <link rel="stylesheet" href="../static/theme.css?v=0.7.3" type="text/css" /><link rel="stylesheet" href="../static/copybutton.css" type="text/css" />
      <link rel="index" title="Index" href="../genindex.html"/>
      
    <link rel="top" title="DeZero Book" href="index.html"/>
      <link rel="next" title="Step 5: Theory of Backpropagation" href="step05.html"/>
      <link rel="prev" title="Step 3: Connecting Functions" href="step03.html"/>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-33864994-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-33864994-1');
</script>
  <script type="text/javascript" id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
    <script src="../static/jquery.js"></script>
    <script src="../static/underscore.js"></script>
    <script src="../static/doctools.js"></script>
    <script src="../static/language_data.js"></script>
    <script src="../static/clipboard.min.js"></script>
    <script src="../static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="DeZero Book">
  <meta property="og:title" content="Step 4: Numerical Differentiation">
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:site" content="@SaitohKoki" />
  <meta property="og:url" content="https://koki0702.github.io/dezero-book/en/step04.html" />
  <meta property="og:image" content="https://koki0702.github.io/dezero-book/images/summary.png" />
</head>
<body role="document" data-page="en/step04">
  <header class="t-head">
    <div class="t-head_menu"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M64 384h384v-42.666H64V384zm0-106.666h384v-42.667H64v42.667zM64 128v42.665h384V128H64z"/></svg></div>
    <a class="t-head_logo" href="index.html">DeZero Book
    </a>
  </header>
  <aside class="t-sidebar">
    <div class="t-sidebar_close">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M405 136.798L375.202 107 256 226.202 136.798 107 107 136.798 226.202 256 107 375.202 136.798 405 256 285.798 375.202 405 405 375.202 285.798 256z"/></svg>
    </div>
    <div class="inner">
<a class="logo" href="index.html">
    DeZero Book
</a>
<div class="github_wrap">
  <a class="github" href="../ja/step04.html">
      日本語
  </a>
  <p class="github_on" href="#">
      English
  </p>
</div><div class="globaltoc">
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="step00.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="part01.html">Stage 1: Automatically compute derivatives</a><ul>
<li class="toctree-l1"><a class="reference internal" href="step01.html">Step 1: Variables as boxes</a></li>
<li class="toctree-l1"><a class="reference internal" href="step02.html">Step 2: Function to create a variable</a></li>
<li class="toctree-l1"><a class="reference internal" href="step03.html">Step 3: Connecting Functions</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Step 4: Numerical Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="step05.html">Step 5: Theory of Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="step06.html">Step 6: Backpropagation by hand</a></li>
<li class="toctree-l1"><a class="reference internal" href="step07.html">Step 7: Automation of backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="step08.html">Step 8: From Recursion to Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="step09.html">Step 9: Making Functions More Useful</a></li>
<li class="toctree-l1"><a class="reference internal" href="step10.html">Step 10: Perform the test</a></li>
<li class="toctree-l1"><a class="reference internal" href="column01.html">Column: Automatic Differentiation</a></li>
</ul>
</li></ul>
</div>

    </div>
  </aside>
  <div class="t-content">
    <div class="t-body yue">
      
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container,
div.nbinput.container div.prompt,
div.nbinput.container div.input_area,
div.nbinput.container div[class*=highlight],
div.nbinput.container div[class*=highlight] pre,
div.nboutput.container,
div.nboutput.container div.prompt,
div.nboutput.container div.output_area,
div.nboutput.container div[class*=highlight],
div.nboutput.container div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    min-width: 5.5ex;
    padding-top: 0.3rem;
    padding-right: 0.3rem;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 0.3rem;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<p class="colab-button"><a href="https://colab.research.google.com/github/koki0702/dezero-book/blob/master/en/step04.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<h1>Step 4: Numerical Differentiation<a class="headerlink" href="#Step-4:-Numerical-Differentiation" title="Permalink to this headline">¶</a></h1>
<p><strong>The code implemented in the previous step</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">class</span> <span class="nc">Variable</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>


<span class="k">class</span> <span class="nc">Function</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">data</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">Square</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>


<span class="k">class</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="Step-4:-Numerical-Differentiation">

<p>We have implemented the <code class="docutils literal notranslate"><span class="pre">Variable</span></code> and <code class="docutils literal notranslate"><span class="pre">Function</span></code> classes in the previous step. In fact, the reason why we bothered to implement such a class is to get the derivative automatically. In this section, we will first review derivative and then try to compute derivative in a simple way called numerical differentiation. The next step is to implement a more efficient alternative to numerical differentiation – <strong>backpropagation</strong> – which is a more efficient algorithm.</p>
<div class="admonition note">
<p class="admonition-title">NOTE</p>
<p>Derivatives are important in many areas, not just machine learning. Derivatives are needed in many fields, such as fluid mechanics, financial engineering, meteorological simulation and engineering design optimization. And in various such fields, the ability to automatically compute derivatives is actually used.</p>
</div>
<div class="section" id="4.1-Derivative">
<h2>4.1 Derivative<a class="headerlink" href="#4.1-Derivative" title="Permalink to this headline">¶</a></h2>
<p>What is a derivative? A derivative is, simply put, a “rate of change”. For example, the rate of change in the position of an object with respect to time - the derivative of the position - is the velocity. Also, the rate of change in velocity with respect to time - the derivative of velocity - corresponds to acceleration. As you can see, the derivative represents the rate of change. And it is defined as the amount of change in an “extremely small amount of time”. In mathematical terms, given the
function <span class="math notranslate nohighlight">\(f(x)\)</span>, the derivative at <span class="math notranslate nohighlight">\(x\)</span> is defined by the following expression</p>
<div class="math notranslate nohighlight">
\[f'(x) = \lim_{ h \to 0} \frac{f(x + h) - f(x)}{h}\tag{4.1}\]</div>
<p>The <span class="math notranslate nohighlight">\(\displaystyle\lim_{h \to 0}\)</span> in equation (4.1) represents the limit, which means to make <span class="math notranslate nohighlight">\(h\)</span> as close as possible to <span class="math notranslate nohighlight">\(0\)</span>. Here, <span class="math notranslate nohighlight">\(\frac{f(x + h) - f(x)}{h}\)</span> in equation (4.1) is the slope of a straight line passing through two points, as shown in <strong>Figure 4-1</strong>.</p>
<p align="center"><img alt="img1-5" src="../images/1-5.png" /></p>
<p align="center"><strong>Figure 4-1</strong> A line through the curve <span class="math notranslate nohighlight">\(y=f(x)\)</span> and its two points</p>
<p>As shown in <strong>Figure 4-1</strong>, the fraction of change in the function <span class="math notranslate nohighlight">\(f(x)\)</span> at two points in <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x+h\)</span> is <span class="math notranslate nohighlight">\(\frac{f(x+h) - f(x)}{h}\)</span>. Now, by making the width of <span class="math notranslate nohighlight">\(h\)</span> as close as possible to zero, we can find the rate of change in <span class="math notranslate nohighlight">\(x\)</span>. This is the derivative of <span class="math notranslate nohighlight">\(y=f(x)\)</span>. Also, if <span class="math notranslate nohighlight">\(y=f(x)\)</span> is a differentiable interval, then Eq. (4.1) holds for “any <span class="math notranslate nohighlight">\(x\)</span>” in that interval. Therefore, <span class="math notranslate nohighlight">\(f'(x)\)</span> in Eq. (4.1) is also a function, and it is
called a <strong>derivative</strong> of <span class="math notranslate nohighlight">\(f(x)\)</span>.</p>
<p>As shown in <strong>Figure 4-1</strong>, the fraction of change in the function <span class="math notranslate nohighlight">\(f(x)\)</span> at two points in <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x+h\)</span> is <span class="math notranslate nohighlight">\(frac\frac{f(x+h) - f(x)}{h}\)</span>. Now, by making the width of <span class="math notranslate nohighlight">\(h\)</span> as close as possible to zero, we can find the rate of change in <span class="math notranslate nohighlight">\(x\)</span>. This is the derivative of <span class="math notranslate nohighlight">\(y=f(x)\)</span>. Also, if <span class="math notranslate nohighlight">\(y=f(x)\)</span> is a differentiable interval, then Eq. (4.1) holds for “any <span class="math notranslate nohighlight">\(x\)</span>” in that interval. Therefore, <span class="math notranslate nohighlight">\(f'(x)\)</span> in Eq. (4.1) is also a function.</p>
</div>
<div class="section" id="4.2-Implementation-of-Numerical-Differentiation">
<h2>4.2 Implementation of Numerical Differentiation<a class="headerlink" href="#4.2-Implementation-of-Numerical-Differentiation" title="Permalink to this headline">¶</a></h2>
<p>Let’s implement the derivative according to the expression (4.1), which is the definition of the derivative. The caveat here is that computers can’t handle extremes. So, we’ll represent <span class="math notranslate nohighlight">\(h\)</span> as an approximation. For example, we compute equation (4.1) using a small value such as <span class="math notranslate nohighlight">\(h = 0.0001\)</span> (= <code class="docutils literal notranslate"><span class="pre">1e-4</span></code>). The method to find the amount of change in a function using such small differences is called <strong>numerical differentiation</strong>.</p>
<p>The numerical derivative approximates the “true derivative” by using a tiny value <span class="math notranslate nohighlight">\(h\)</span>. As a result, the value is subject to error. There is a technique called “central difference approximation” to reduce the approximation error. In the central difference approximation, instead of finding the difference between <span class="math notranslate nohighlight">\(f(x)\)</span> and <span class="math notranslate nohighlight">\(f(x+h)\)</span>, we find the difference between <span class="math notranslate nohighlight">\(f(x-h)\)</span> and <span class="math notranslate nohighlight">\(f(x+h)\)</span>. As shown in the figure, it looks like the blue line in <strong>Figure 4-2</strong>.</p>
<p align="center"><img alt="img1-6en" src="../images/1-6en.png" /></p>
<p align="center"><strong>Figure 4-2</strong> Comparison of “true derivative”, “forward difference approximation” and “central difference approximation”</p>
<p>As shown in <strong>Figure 4-2</strong>, the method of finding the slope at two points, <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x+h\)</span>, is called the “forward difference approximation”. The case of <span class="math notranslate nohighlight">\(x-h\)</span> and <span class="math notranslate nohighlight">\(x+h\)</span> is called the “central difference approximation”, and it is known that the error is less in this case. We won’t prove it here, but intuitively it can be seen from the slope of the straight line in <strong>Figure 4-2</strong>. Note that the slope of the line in the central difference approximation is
<span class="math notranslate nohighlight">\(\frac{f(x+h) - f(x-h)}{2h}\)</span> (note that the denominator is <span class="math notranslate nohighlight">\(2h\)</span>).</p>
<div class="admonition note">
<p class="admonition-title">NOTE</p>
<p>It can be proved by the Taylor expansion that the central difference approximation is closer to the true derivative than the forward difference approximation. For proof of this, see literature <a href="http://www.ohiouniversityfaculty.com/youngt/IntNumMeth/lecture27.pdf">[1]</a>.</p>
</div>
<p>Let’s implement the function <code class="docutils literal notranslate"><span class="pre">numerical_diff(f,</span> <span class="pre">x,</span> <span class="pre">eps=1e-4)</span></code> to compute a numerical derivative using the central difference approximation. The argument <code class="docutils literal notranslate"><span class="pre">f</span></code> gives a <code class="docutils literal notranslate"><span class="pre">Function</span></code> instance of the function to be differentiated. The argument <code class="docutils literal notranslate"><span class="pre">x</span></code> gives the variable for the differentiation in a <code class="docutils literal notranslate"><span class="pre">Variable</span></code> instance. The default value is <code class="docutils literal notranslate"><span class="pre">1e-4</span></code> (=``0.0001``), where <code class="docutils literal notranslate"><span class="pre">eps</span></code> is a small value. Then, the numerical derivative can be implemented as follows.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">y0</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y1</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">y0</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eps</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>If we note above that the instance variable <code class="docutils literal notranslate"><span class="pre">data</span></code> of <code class="docutils literal notranslate"><span class="pre">Variable</span></code> contains the actual data, there is nothing else to be concerned about. Now, let’s actually compute the derivative for the <code class="docutils literal notranslate"><span class="pre">Square</span></code> class that we implemented in step 3.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">f</span> <span class="o">=</span> <span class="n">Square</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">2.0</span><span class="p">))</span>
<span class="n">dy</span> <span class="o">=</span> <span class="n">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
4.000000000004
</pre></div></div>
</div>
<p>As the results above show, the derivative of <span class="math notranslate nohighlight">\(y=x^2\)</span> in <span class="math notranslate nohighlight">\(x=2.0\)</span> is <span class="math notranslate nohighlight">\(4.000000000004\)</span>. The exact value of the derivative without error is <span class="math notranslate nohighlight">\(4.0\)</span>, so the result of this run is approximately correct.</p>
<div class="admonition warning">
<p class="admonition-title">WARNING</p>
<p>Calculations of the derivatives can also be solved analytically. To solve analytically is to derive an answer only by a variation of a formula. In the above example, the formula for the derivative gives <span class="math notranslate nohighlight">\(\frac{dy}{dx} = 2x\)</span> for <span class="math notranslate nohighlight">\(y=x^2\)</span> (<span class="math notranslate nohighlight">\(\frac{dy}{dx}\)</span> is the symbol for the derivative of <span class="math notranslate nohighlight">\(y\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>). Therefore, the derivative in <span class="math notranslate nohighlight">\(x=2.0\)</span> is <span class="math notranslate nohighlight">\(4.0\)</span>. This value of <span class="math notranslate nohighlight">\(4.0\)</span> is the exact value that does not include errors. The result of the
numerical derivative above isn’t exactly <span class="math notranslate nohighlight">\(4.0\)</span>, but you can see that the error is quite small.</p>
</div>
</div>
<div class="section" id="4.3-Derivatives-of-composite-functions">
<h2>4.3 Derivatives of composite functions<a class="headerlink" href="#4.3-Derivatives-of-composite-functions" title="Permalink to this headline">¶</a></h2>
<p>So far we have dealt with a simple function <span class="math notranslate nohighlight">\(y=x^2\)</span>. Next, let’s find the derivative of the composite function. Here, for <span class="math notranslate nohighlight">\(y = (e^{x^2})^2\)</span>, we find the derivative <span class="math notranslate nohighlight">\(\frac{dy}{dx}\)</span>. The code looks like this.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">Square</span><span class="p">()</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">Exp</span><span class="p">()</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">Square</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">C</span><span class="p">(</span><span class="n">B</span><span class="p">(</span><span class="n">A</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">dy</span> <span class="o">=</span> <span class="n">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
3.2974426293330694
</pre></div></div>
</div>
<p>In the above code, we have a series of calculations in a function called <code class="docutils literal notranslate"><span class="pre">f</span></code>. Since functions are also objects in Python, we can pass a function as an object to the arguments of other functions. In the above example, we actually pass the function <code class="docutils literal notranslate"><span class="pre">f</span></code> to the <code class="docutils literal notranslate"><span class="pre">numerical_diff</span></code> function.</p>
<p>Looking at the result above, the derivative is <code class="docutils literal notranslate"><span class="pre">3.297...</span> <span class="pre">The</span> <span class="pre">derivative</span> <span class="pre">is</span> <span class="pre">now</span></code>3.297… This means that if we change <code class="docutils literal notranslate"><span class="pre">x</span></code> from <code class="docutils literal notranslate"><span class="pre">0.5</span></code> to only a small value, the value of <code class="docutils literal notranslate"><span class="pre">y</span></code> will change by <code class="docutils literal notranslate"><span class="pre">3.297...</span></code> times that small value. This means that if you change only a small value from <code class="docutils literal notranslate"><span class="pre">0.5</span></code>, then the value of <code class="docutils literal notranslate"><span class="pre">y</span></code> will change only by <code class="docutils literal notranslate"><span class="pre">3.297...</span></code> times that small value.</p>
<p>Now we have succeeded in finding the derivative “automatically”. Once the desired calculation was expressed in code (in the example above, we defined the function <code class="docutils literal notranslate"><span class="pre">f</span></code>), the derivative was computed automatically by the program. If we follow this method, we can find the derivative automatically, no matter how complex the functions are combined! All that’s left to do is to increase the number of functions, and you can get the value of the derivative of any calculation–if it’s a differentiable
function. Unfortunately, however, there is a problem with numerical differentiation.</p>
</div>
<div class="section" id="4.4-Problems-with-Numerical-Differentiation">
<h2>4.4 Problems with Numerical Differentiation<a class="headerlink" href="#4.4-Problems-with-Numerical-Differentiation" title="Permalink to this headline">¶</a></h2>
<p>The results of the numerical differentiation are subject to error. In many cases, the error is very small, but some calculations may include large errors.</p>
<div class="admonition note">
<p class="admonition-title">NOTE</p>
<p>The main reason why errors tend to be included in the results of numerical differentiation is due to “cancellation of significant digits”. In the calculation for “difference”, such as the center-difference approximation, the difference is taken from numbers of the same magnitude, but the number of effective digits decreases in the result of the calculation due to cancellation of significant digits. For example, let’s consider the calculation of <span class="math notranslate nohighlight">\(1.234 - 1.233\)</span> when the number of valid
digits is 4 - subtraction of close values. The result will be <span class="math notranslate nohighlight">\(0.001\)</span> and the number of valid digits will be one. By the same principle, the calculation of the difference in the numerical derivative will cause the digits to fall off, so the error is more likely to be included.</p>
</div>
<p>A more serious problem with numerical differentiation is that it is computationally expensive. More specifically, if you want to find the derivative for multiple variables, you need to find the derivative for each variable. In neural networks, there are cases where the derivative is obtained for more than several million variables (parameters), and it is impractical to obtain the derivative of that number using a numerical differention. In its place is back-propagation. From the next step, we
finally move on to back-propagation.</p>
<p>Note that the numerical derivative is easy to implement and gives the approximate correct value. Back-propagation, on the other hand, is a complex algorithm, and its implementation is easily riddled with bugs. So, to check the correctness of the back-propagation implementation, we will use the results of the numerical differentiation. This is called <strong>gradient check</strong>, which is a method of comparing the results of a numerical differentiation with the results of a back-propagation. The gradient
check is implemented in “Step 10”.</p>
</div>
</div>

  <div class="t-pagination clearfix">
    <span>
      ← <a href="step03.html" title="Step 3: Connecting Functions">Step 3: Connecting Functions</a>
    </span>
    <span style="float:right">
      <a href="step05.html" title="Step 5: Theory of Backpropagation">Step 5: Theory of Backpropagation</a> →
    </span>
  </div>

    </div><footer class="t-foot">
    &copy; Copyright 2020, Koki Saitoh.
    <br>
    A <a href="https://typlog.com/">typlog</a> <a href="https://github.com/typlog/sphinx-typlog-theme">sphinx theme</a>,
    designed by <a href="https://lepture.com/">Hsiaoming Yang</a>.
</footer>
  </div>
  <script>$(function(){$(".t-head_menu").on("click",function(){$("body").addClass("_expand")});$(".t-body").on("click",function(){$("body").removeClass("_expand")});$(".t-sidebar_close").on("click",function(){$("body").removeClass("_expand")});$("a.footnote-reference").on("click",function(e){e.preventDefault();var id=$(this).attr("href");var html=$(id).find("td.label + td").html();var w=Math.max(document.documentElement.clientWidth,window.innerWidth||0);var style="top:"+e.pageY+"px;";if(w>560){style+="width:480px;";if(e.pageX>240&&e.pageX+240<w){style+="left:"+(e.pageX-240)+"px;"}else if(e.pageX<=240){style+="left:20px;"}else{style+="right:20px;"}}showFootnote(html,style)});function showFootnote(html,style){var CONTENT_ID="typlog-footnote-content";var content=document.getElementById(CONTENT_ID);if(!content){content=document.createElement("div");content.id=CONTENT_ID;$(".t-body").append(content)}var MASK_ID="typlog-footnote-mask";var mask=document.getElementById(MASK_ID);if(!mask){mask=document.createElement("div");mask.id=MASK_ID;document.body.appendChild(mask);mask.addEventListener("click",function(){content.className="";mask.className=""})}content.innerHTML=html;content.setAttribute("style",style);content.className="_active";mask.className="_active"}function fetchGitHubRepo(repo){var url="https://api.github.com/repos/"+repo;$.getJSON(url,function(data){var counts=[+new Date,data.stargazers_count,data.forks_count];localStorage.setItem("gh:"+repo,JSON.stringify(counts));updateGitHubStats(counts[1],counts[2])})}function updateGitHubStats(stars,forks){$(".github_stars strong").text(stars);$(".github_forks strong").text(forks)}function initGitHub(url){if(!url){return}var repo=url.replace("https://github.com/","");var cache=localStorage.getItem("gh:"+repo);if(cache){try{var counts=JSON.parse(cache);updateGitHubStats(counts[1],counts[2]);var delta=new Date-counts[0];if(delta<0||delta>9e5){fetchGitHubRepo(repo)}}catch(error){fetchGitHubRepo(repo)}}else{fetchGitHubRepo(repo)}}initGitHub($(".github").attr("href"))});</script>
</body>
</html>